
services:

  zookeeper:
    image: confluentinc/cp-zookeeper:7.7.1
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-log:/var/lib/zookeeper/log
    networks:
      - etl_network

  kafka:
    image: confluentinc/cp-kafka:7.7.1
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - kafka-data:/var/lib/kafka/data
    networks:
      - etl_network

  cassandra:
    image: cassandra:4.1.6
    ports:
      - "9042:9042"
    volumes:
      - cassandra_data:/var/lib/cassandra
    environment:
      - CASSANDRA_CLUSTER_NAME=MyCluster
    deploy:
      resources:
        limits:
          memory: 3G
    networks:
      - etl_network


  mysql:
    image: mysql:8.0
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./mysql-init-scripts:/docker-entrypoint-initdb.d
    command: --default-authentication-plugin=caching_sha2_password
    environment:
      - MYSQL_ROOT_PASSWORD=rootpassword
      - MYSQL_DATABASE=mydatabase
      - MYSQL_USER=myuser
      - MYSQL_PASSWORD=mypassword
    networks:
      - etl_network

  pyspark:
    build: 
      context: .
      dockerfile: Dockerfile.pyspark
    hostname: pyspark
    container_name: pyspark
    depends_on:
      - mysql
      - cassandra
    environment:
      SPARK_MODE: master
    ports:
      - "8080:8080"   # Spark UI
      - "7077:7077"   # Spark Master Port
    volumes:
      - /home/hien2706/hien_code/de_project/spark_job:/app/spark_job  
    networks:
      - etl_network

  python-source:
    build:
      context: .
      dockerfile: Dockerfile.python
    volumes:
      - ./python-scripts:/app/python-scripts
    command: tail -f /dev/null
    depends_on:
      - kafka
      - cassandra
    networks:
      - etl_network

  python-sink:
    build:
      context: .
      dockerfile: Dockerfile.python
    volumes:
      - ./python-scripts:/app/python-scripts
    command: tail -f /dev/null
    depends_on:
      - kafka
      - cassandra
    networks:
      - etl_network

  airflow:
    image: apache/airflow:2.10.1
    user: "${AIRFLOW_UID:-50000}:0"
    restart: always
    depends_on:
      - pyspark
      - python-source
      - python-sink
      - mysql
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=mysql://airflow:airflow@mysql:3306/airflow
      - AIRFLOW__WEBSERVER__WEB_SERVER_PORT=8081
    ports:
      - "8081:8081"  # Airflow webserver port
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./config:/opt/airflow/config
      - /var/run/docker.sock:/var/run/docker.sock
    command: >
      bash -c "
      airflow db init &&
      airflow db upgrade &&
      airflow users create
      --username admin
      --password admin
      --firstname hien
      --lastname tran
      --role Admin
      --email hien27062004@gmail.com &&
      airflow webserver & airflow scheduler
      "
    networks:
      - etl_network


  grafana:
    image: grafana/grafana:11.2.0
    ports:
    - "3000:3000"
    volumes:
    - grafana-data:/var/lib/grafana
    environment:
    - GF_SECURITY_ADMIN_USER=admin
    - GF_SECURITY_ADMIN_PASSWORD=admin
    networks:
    - etl_network
    depends_on:
    - mysql
    - cassandra

volumes:
  cassandra_data:
  mysql_data:
  kafka-data:
  zookeeper-data:
  zookeeper-log:
  grafana-data:


networks:
  etl_network:
    external: true

